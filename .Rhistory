missing_values <- data.frame(
#Variable = names(BCN_Accomm_full),
Missing_Count = colSums(is.na(BCN_Accomm_sub)),
Missing_Percent = paste0(round((colSums(is.na(BCN_Accomm_sub)) / total_rows) * 100, 2), "%"))
# Order DataFrame table in descending order
missing_values <- missing_values[order(-missing_values$Missing_Count), ]
# Display the table
knitr::kable(missing_values,
caption = "Missing Values by Variable",
align = "c")
# Create a new data frame with the imputed review_scores_rating column
imp_BCN_Accomm_sub <- BCN_Accomm_sub %>%
mutate(review_scores_rating = if_else(is.na(review_scores_rating), 0, review_scores_rating))
md.pattern(imp_BCN_Accomm_sub, rotate.names = TRUE)
BCN_Accomm <- na.omit(imp_BCN_Accomm_sub) # from this point we'll use this data set
md.pattern(BCN_Accomm, rotate.names = TRUE)
# Create a correlation matrix for numeric fields
cor_BNC_Accomm <- select_if(BCN_Accomm, is.numeric) %>%
select(-c(id, X, host_id))
# make a data frame
cor_BNC_Accomm <- data.frame(cor_BNC_Accomm)
str(cor_BNC_Accomm)
# print correlation matrix
corrplot(cor(cor_BNC_Accomm), type = "upper", order = "hclust", tl.col = "black")
# Calculate Q1, Q3, and IQR for the price variable
Q1 <- quantile(BCN_Accomm$price, 0.25)
Q3 <- quantile(BCN_Accomm$price, 0.75)
IQR <- Q3 - Q1
# Add a column to indicate outliers
BCN_Accomm_sub_outlier <- BCN_Accomm %>%
mutate(is_outlier = price < (Q1 - 1.5 * IQR) | price > (Q3 + 1.5 * IQR))
# Count the number of outliers
outliers_count <- sum(BCN_Accomm_sub_outlier$is_outlier)
# Display outliers count
outliers_count
# Boxplot for price variable
boxplot(BCN_Accomm$price,
horizontal = TRUE,
axes = FALSE,
staplewex = 1,
ylim = c(2, 500),
main = "Boxplot of Price/Night Variable (€)")
# Filter rows Outliers
outlier_extremes <- BCN_Accomm_sub_outlier%>%
filter(is_outlier == TRUE) %>%
select(neighbourhood, property_type, bedrooms, price) %>%
arrange(desc(price), neighbourhood, property_type) %>%
slice_head(n = 20)  # Select top 20 Outliers for price
# Display Prices outliers
knitr::kable(outlier_extremes,
caption = "Table 2: Top 20 and Outliers for Price Variable with Neighbourhood",
align = "c")
# Filter dataset
BCN_Accomm <- BCN_Accomm %>%
filter(price <= 1000)
summary(BCN_Accomm$price)
# Calculate percentages and relabel values
superhost_data <- BCN_Accomm %>%
count(host_is_superhost) %>%
mutate(
percentage = n / sum(n) * 100,  # Calculate percentages
host_is_superhost = recode(host_is_superhost, "f" = "False", "t" = "True")  # Relabel
)
# Create the pie chart with percentages
pc_sh_status <- ggplot(superhost_data, aes(x = "", y = n, fill = host_is_superhost)) +
geom_col() +
coord_polar(theta = "y") +
geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5)) +  # Add percentages
labs(
title = "Proportion of Superhost Status",
fill = "Superhost Status\n(False = Not a Superhost, True = Superhost)"
) +
theme_void() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),  # Center and bold title
legend.title = element_text(size = 10),
legend.text = element_text(size = 9)
)
print(pc_sh_status)
# Create a new column for occupancy rate
BCN_Accomm$occupancy_rate_30 <- (1 - BCN_Accomm$availability_30 / 30) * 100
# Preview the new column
head(BCN_Accomm$occupancy_rate_30)
# Remove rows with the unwanted neighborhoods
BCN_Accomm <- BCN_Accomm[!(BCN_Accomm$neighbourhood %in% c("La Vall d'Hebron", "Torre Baró")), ]
# converting categorical variables into factors
BCN_Accomm$neighbourhood <- as.factor(BCN_Accomm$neighbourhood)
BCN_Accomm$property_type <- as.factor(BCN_Accomm$property_type)
BCN_Accomm$room_type <- as.factor(BCN_Accomm$room_type)
BCN_Accomm$zipcode <- as.factor(BCN_Accomm$zipcode)
BCN_Accomm$availability_30 <- as.factor(BCN_Accomm$availability_30)
# Convert variables that represent counts or continuous numbers into numeric
# If these were read as characters or factors, ensure they're numeric:
if (is.factor(BCN_Accomm$accommodates)) {
BCN_Accomm$accommodates <- as.numeric(as.character(BCN_Accomm$accommodates))
}
if (is.factor(BCN_Accomm$bathrooms)) {
BCN_Accomm$bathrooms <- as.numeric(as.character(BCN_Accomm$bathrooms))
}
if (!is.numeric(BCN_Accomm$bedrooms)) {
BCN_Accomm$bedrooms <- as.numeric(BCN_Accomm$bedrooms)
}
if (!is.numeric(BCN_Accomm$beds)) {
BCN_Accomm$beds <- as.numeric(BCN_Accomm$beds)
}
if (!is.numeric(BCN_Accomm$latitude)) {
BCN_Accomm$latitude <- as.numeric(BCN_Accomm$latitude)
}
if (!is.numeric(BCN_Accomm$longitude)) {
BCN_Accomm$longitude <- as.numeric(BCN_Accomm$longitude)
}
if (!is.numeric(BCN_Accomm$review_scores_rating)) {
BCN_Accomm$review_scores_rating <- as.numeric(BCN_Accomm$review_scores_rating)
}
if (!is.numeric(BCN_Accomm$minimum_nights)) {
BCN_Accomm$minimum_nights <- as.numeric(BCN_Accomm$minimum_nights)
}
# Ensure zipcodes are cleaned or set unknowns
BCN_Accomm$zipcode <- ifelse(grepl("^[0-9]{4}$", BCN_Accomm$zipcode),
BCN_Accomm$zipcode,
"unknown")
BCN_Accomm$zipcode <- as.factor(BCN_Accomm$zipcode)
# Replace invalid zipcodes with "unknown"
BCN_Accomm$zipcode <- ifelse(
grepl("^[0-9]{4}$", BCN_Accomm$zipcode),   # Check if zipcode is exactly 4 digits
BCN_Accomm$zipcode,                        # Keep valid zipcodes
"unknown")                                # Replace invalid zipcodes with "unknown"
set.seed(1000)
# Define the number of groups and the amount of sample for each
group <- sample(2, nrow(BCN_Accomm),
replace = TRUE,
prob = c(0.4, 0.4))
# training data set with around 60% of the samples
train <- BCN_Accomm[group==1,]
# test data set with around 40% of the samples
test <- BCN_Accomm[group==2,]
test <- na.omit(test)
# For all factor variables used in the model, ensure that the test factors have the same levels as train
test$property_type <- factor(test$property_type, levels = levels(train$property_type))
test$room_type <- factor(test$room_type, levels = levels(train$room_type))
test$neighbourhood <- factor(test$neighbourhood, levels = levels(train$neighbourhood))
# Ensure no NAs or infinite values in important numeric variables
numeric_vars <- c("bathrooms", "bedrooms", "accommodates", "beds",
"latitude", "longitude", "review_scores_rating", "minimum_nights")
# Bedrooms vs Price
ggplot(BCN_Accomm, aes(x = review_scores_rating, y = price)) +
geom_point(color = "blue") +
geom_smooth(method = "loess", color = "red") + # Loess: on-parametric method for fitting a curve to the data.
labs(title = "Price vs Review Score Ratings", x = "Review Score", y = "Price")+
theme(
legend.position = "none",
plot.title = element_text(hjust = 0.5, face = "bold"))
# Gam Model
gam_price <- gam(price ~
s(bathrooms) + s(bedrooms) + s(accommodates) + s(beds) +
s(latitude) + s(longitude) +
s(review_scores_rating) + s(minimum_nights) +
room_type + neighbourhood,
data = train, method = "REML")
# Summary of the occupancy model
summary(gam_price)
# Make sure test has no problematic values before predicting
test$predicted_price <- predict(gam_price, newdata = test)
# Calculate R-squared on the test set
r_squared <- 1 - sum((test$price - test$predicted_price)^2) / sum((test$price - mean(test$price))^2)
cat("R-squared on Test Set: ", r_squared, "\n")
# Plot observed vs predicted with points in blue
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +  # Set points color to blue
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Test Set)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
install.packages("neuralnet")
# Load required libraries
library(dplyr)        # Data manipulation and wrangling
library(ggplot2)      # Data visualization
library(tidyr)        # Data tidying
library(RColorBrewer) # Color palettes for visualizations
library(leaflet)      # Interactive maps
library(shiny)        # Building interactive web applications
library(leaflet.extras) # Additional features for leaflet maps
library(corrplot)     # Visualization of correlation matrices
library(mice)         # Handling missing data
library(nnet)         # Multinomial regression model
library(forcats)      # Handling categorical variables
library(patchwork)    # Combining multiple ggplots
library(VGAM)         # Multinomial logistic regression models
library(tidyverse)    # Collection of data science packages
library(e1071)        # Support Vector Machines (SVM)
library(reshape2)     # Data reshaping for ggplot visualizations
library(caret)        # Machine learning and model evaluation
library(wordcloud)    # Wordcloud plot
library(rsample)      # Split dataset into training and testing
library(mgcv)         # Gam Mdels
library(neuralnet)    # Neural Netwotk
# Normalize data
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Apply normalization to training data
train_normalized <- as.data.frame(lapply(train[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Formula
formula_nn <- price ~ bathrooms + bedrooms + accommodates + beds +
latitude + longitude + review_scores_rating + minimum_nights
# Train the Neural Network
nn_price <- neuralnet(formula_nn,
data = train_normalized,
hidden = c(5, 3),  # 2 hidden layers with 5 and 3 neurons
linear.output = TRUE)
# Plot
plot(nn_price)
# Calculate errors
mae <- mean(abs(test$availability_30 - test$predicted_occupancy_30))
rmse <- sqrt(mean((test$availability_30 - test$predicted_occupancy_30)^2))
r_squared <- 1 - sum((test$availability_30 - test$predicted_occupancy_30)^2) / sum((test$price - mean(test$availability_30))^2)
cat("MAE: ", mae, "\n")
cat("RMSE: ", rmse, "\n")
cat("R-squared: ", r_squared, "\n")
# Normalize data
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Apply normalization to training data
train_normalized <- as.data.frame(lapply(train[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Formula
formula_nn <- price ~ bathrooms + bedrooms + accommodates + beds +
latitude + longitude + review_scores_rating + minimum_nights
# Train the Neural Network
nn_price <- neuralnet(formula_nn,
data = train_normalized,
hidden = c(5, 3),  # 2 hidden layers with 5 and 3 neurons
linear.output = TRUE)
# Plot
plot(nn_price)
# Summary
summary(nn_price)
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
# Calculate errors
mae_nn <- mean(abs(test$price - test$predicted_price))
rmse_nn <- sqrt(mean((test$price - test$predicted_price)^2))
r_squared_nn <- 1 - sum((test$price - test$predicted_price)^2) / sum((test$price - mean(test$price))^2)
cat("MAE: ", mae_nn, "\n")
cat("RMSE: ", rmse_nn, "\n")
cat("R-squared: ", r_squared_nn, "\n")
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal() +
ggsave("Observed_vs_Predicted_Prices.png")
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
# Make sure test has no problematic values before predicting
test$predicted_price <- predict(gam_price, newdata = test)
# Calculate R-squared on the test set
r_squared <- 1 - sum((test$price - test$predicted_price)^2) / sum((test$price - mean(test$price))^2)
cat("R-squared on Test Set: ", r_squared, "\n")
# Plot observed vs predicted with points in blue
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +  # Set points color to blue
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Test Set)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
# Load required libraries
library(dplyr)        # Data manipulation and wrangling
library(ggplot2)      # Data visualization
library(tidyr)        # Data tidying
library(RColorBrewer) # Color palettes for visualizations
library(leaflet)      # Interactive maps
library(shiny)        # Building interactive web applications
library(leaflet.extras) # Additional features for leaflet maps
library(corrplot)     # Visualization of correlation matrices
library(mice)         # Handling missing data
library(nnet)         # Multinomial regression model
library(forcats)      # Handling categorical variables
library(patchwork)    # Combining multiple ggplots
library(VGAM)         # Multinomial logistic regression models
library(tidyverse)    # Collection of data science packages
library(e1071)        # Support Vector Machines (SVM)
library(reshape2)     # Data reshaping for ggplot visualizations
library(caret)        # Machine learning and model evaluation
library(wordcloud)    # Wordcloud plot
library(rsample)      # Split dataset into training and testing
library(mgcv)         # Gam Mdels
library(neuralnet)    # Neural Netwotk
# Normalize the test data
test_normalized <- as.data.frame(lapply(test[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Prediction
nn_predictions <- compute(nn_price,
test_normalized[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights")])
# Denormalize the predictions
denormalize <- function(x, original_min, original_max) {
x * (original_max - original_min) + original_min
}
test$predicted_price <- denormalize(nn_predictions$net.result,
min(train$price),
max(train$price))
# Plot observed vs predicted prices
ggplot(test, aes(x = price, y = predicted_price)) +
geom_point(alpha = 0.5, color = "blue") +
geom_abline(slope = 1, intercept = 0, color = "red") +
labs(title = "Observed vs Predicted Prices (Neural Network)",
x = "Observed Price", y = "Predicted Price") +
theme_minimal()
summary(train_normalized)
sum(is.na(train[, c("bathrooms", "bedrooms", "accommodates", "beds",
"latitude", "longitude", "review_scores_rating", "minimum_nights", "price")]))
# Normalize data
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Apply normalization to training data
train_normalized <- as.data.frame(lapply(train[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Formula
formula_nn <- price ~ bathrooms + bedrooms + accommodates + beds +
latitude + longitude + review_scores_rating + minimum_nights
# Train the Neural Network
nn_price <- neuralnet(formula_nn,
data = train_normalized,
hidden = c(5, 3),  # 2 hidden layers with 5 and 3 neurons
linear.output = TRUE)
# Normalize data
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Apply normalization to training data
train_normalized <- as.data.frame(lapply(train[, c("bathrooms", "bedrooms", "accommodates",
"beds", "latitude", "longitude",
"review_scores_rating", "minimum_nights", "price")], normalize))
# Formula
formula_nn <- price ~ bathrooms + bedrooms + accommodates + beds +
latitude + longitude + review_scores_rating + minimum_nights
# Train the Neural Network
nn_price <- neuralnet(formula_nn,
data = train_normalized,
hidden = c(5, 3),  # 2 hidden layers with 5 and 3 neurons
linear.output = TRUE,
stepmax = 1e6)
# Plot
plot(nn_price)
# Summary
summary(nn_price)
nn_price$result.matrix
# Calculate errors
mae_nn <- mean(abs(test$price - test$predicted_price))
rmse_nn <- sqrt(mean((test$price - test$predicted_price)^2))
r_squared_nn <- 1 - sum((test$price - test$predicted_price)^2) / sum((test$price - mean(test$price))^2)
# Create a performance summary table
performance_table <- data.frame(
Metric = c("Mean Absolute Error (MAE)", "Root Mean Squared Error (RMSE)", "R-squared"),
Value = c(mae_nn, rmse_nn, r_squared_nn)
)
# Display the table
knitr::kable(performance_table, format = "html", caption = "Neural Network Model Performance")
